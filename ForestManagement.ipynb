{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
    "from hiive.mdptoolbox.example import forest\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = forest(S=25, r1=10, r2=6, p=0.1)\n",
    "\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "def test_policy(P, R, policy, test_count=1000, gamma=0.9):\n",
    "    num_state = P.shape[-1]\n",
    "    total_episode = num_state * test_count\n",
    "    total_reward = 0\n",
    "    for state in range(num_state):\n",
    "        state_reward = 0\n",
    "        for state_episode in range(test_count):\n",
    "            episode_reward = 0\n",
    "            disc_rate = 1\n",
    "            while True:\n",
    "                action = policy[state]\n",
    "                probs = P[action][state]\n",
    "                candidates = list(range(len(P[action][state])))\n",
    "                next_state =  choice(candidates, 1, p=probs)[0]\n",
    "                reward = R[state][action] * disc_rate\n",
    "                episode_reward += reward\n",
    "                disc_rate *= gamma\n",
    "                if next_state == 0:\n",
    "                    break\n",
    "            state_reward += episode_reward\n",
    "        total_reward += state_reward\n",
    "    return total_reward / total_episode\n",
    "\n",
    "\n",
    "def trainVI(P, R, discount=0.9, epsilon=[1e-9]):\n",
    "    vi_df = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \n",
    "                                  \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for eps in epsilon:\n",
    "        vi = ValueIteration(P, R, gamma=discount, epsilon=eps, max_iter=int(1e15))\n",
    "        vi.run()\n",
    "        reward = test_policy(P, R, vi.policy)\n",
    "        info = [float(eps), vi.policy, vi.iter, vi.time, reward, vi.V]\n",
    "        df_length = len(vi_df)\n",
    "        vi_df.loc[df_length] = info\n",
    "    return vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_df = trainVI(P, R, epsilon=[1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-15])\n",
    "vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PolicyIteration(P, R, gamma=0.9, max_iter=1e6)\n",
    "pi.run()\n",
    "pi_pol = pi.policy\n",
    "pi_reward = test_policy(P, R, pi_pol)\n",
    "pi_iter = pi.iter\n",
    "pi_time = pi.time\n",
    "pi_iter, pi_time, pi_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQ(P, R, discount=0.9, alpha_dec=[0.99], alpha_min=[0.001], \n",
    "            epsilon=[1.0], epsilon_decay=[0.99], n_iter=[1000000]):\n",
    "    q_df = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \n",
    "                                 \"Epsilon\", \"Epsilon Decay\", \"Reward\",\n",
    "                                 \"Time\", \"Policy\", \"Value Function\",\n",
    "                                 \"Training Rewards\"])\n",
    "    \n",
    "    count = 0\n",
    "    for i in n_iter:\n",
    "        for eps in epsilon:\n",
    "            for eps_dec in epsilon_decay:\n",
    "                for a_dec in alpha_dec:\n",
    "                    for a_min in alpha_min:\n",
    "                        q = QLearning(P, R, discount, alpha_decay=a_dec, \n",
    "                                      alpha_min=a_min, epsilon=eps, \n",
    "                                      epsilon_decay=eps_dec, n_iter=i)\n",
    "                        q.run()\n",
    "                        reward = test_policy(P, R, q.policy)\n",
    "                        count += 1\n",
    "                        print(\"{}: {}\".format(count, reward))\n",
    "                        st = q.run_stats\n",
    "                        rews = [s['Reward'] for s in st]\n",
    "                        info = [i, a_dec, a_min, eps, eps_dec, reward, \n",
    "                                q.time, q.policy, q.V, rews]\n",
    "                        \n",
    "                        df_length = len(q_df)\n",
    "                        q_df.loc[df_length] = info\n",
    "    return q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_decs = [0.99, 0.999]\n",
    "alpha_mins =[0.001, 0.0001]\n",
    "eps = [10.0, 1.0]\n",
    "eps_dec = [0.99, 0.999]\n",
    "iters = [1000000, 10000000]\n",
    "q_df = trainQ(P, R, discount=0.9, alpha_dec=alpha_decs, alpha_min=alpha_mins, \n",
    "            epsilon=eps, epsilon_decay=eps_dec, n_iter=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_df.Policy == pi_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy(P,R,q_df.Policy[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.groupby(\"Iterations\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.groupby(\"Epsilon Decay\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "P, R = forest(S=600, r1=100, r2= 15, p=0.01)\n",
    "\n",
    "\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "def test_policy(P, R, policy, test_count=100, gamma=0.9):\n",
    "    num_state = P.shape[-1]\n",
    "    total_episode = num_state * test_count\n",
    "    total_reward = 0\n",
    "    for state in range(num_state):\n",
    "        state_reward = 0\n",
    "        for state_episode in range(test_count):\n",
    "            episode_reward = 0\n",
    "            disc_rate = 1\n",
    "            while True:\n",
    "                action = policy[state]\n",
    "                probs = P[action][state]\n",
    "                candidates = list(range(len(P[action][state])))\n",
    "                next_state =  choice(candidates, 1, p=probs)[0]\n",
    "                reward = R[state][action] * disc_rate\n",
    "                episode_reward += reward\n",
    "                disc_rate *= gamma\n",
    "                if next_state == 0:\n",
    "                    break\n",
    "            state_reward += episode_reward\n",
    "        total_reward += state_reward\n",
    "    return total_reward / total_episode\n",
    "\n",
    "\n",
    "def trainVI(P, R, discount=0.9, epsilon=[1e-9]):\n",
    "    vi_df = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \n",
    "                                  \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for eps in epsilon:\n",
    "        vi = ValueIteration(P, R, gamma=discount, epsilon=eps, max_iter=int(1e15))\n",
    "        vi.run()\n",
    "        reward = test_policy(P, R, vi.policy)\n",
    "        info = [float(eps), vi.policy, vi.iter, vi.time, reward, vi.V]\n",
    "        df_length = len(vi_df)\n",
    "        vi_df.loc[df_length] = info\n",
    "    return vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vi_df = trainVI(P, R, epsilon=[1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-15])\n",
    "vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pi = PolicyIteration(P, R, gamma=0.9, max_iter=1e6)\n",
    "pi.run()\n",
    "pi_pol = pi.policy\n",
    "pi_reward = test_policy(P, R, pi_pol)\n",
    "pi_iter = pi.iter\n",
    "pi_time = pi.time\n",
    "pi_iter, pi_time, pi_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def trainQ(P, R, discount=0.9, alpha_dec=[0.99], alpha_min=[0.001], \n",
    "            epsilon=[1.0], epsilon_decay=[0.99], n_iter=[1000000]):\n",
    "    q_df = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \n",
    "                                 \"Epsilon\", \"Epsilon Decay\", \"Reward\",\n",
    "                                 \"Time\", \"Policy\", \"Value Function\",\n",
    "                                 \"Training Rewards\"])\n",
    "    \n",
    "    count = 0\n",
    "    for i in n_iter:\n",
    "        for eps in epsilon:\n",
    "            for eps_dec in epsilon_decay:\n",
    "                for a_dec in alpha_dec:\n",
    "                    for a_min in alpha_min:\n",
    "                        q = QLearning(P, R, discount, alpha_decay=a_dec, \n",
    "                                      alpha_min=a_min, epsilon=eps, \n",
    "                                      epsilon_decay=eps_dec, n_iter=i)\n",
    "                        q.run()\n",
    "                        reward = test_policy(P, R, q.policy)\n",
    "                        count += 1\n",
    "                        print(\"{}: {}\".format(count, reward))\n",
    "                        st = q.run_stats\n",
    "                        rews = [s['Reward'] for s in st]\n",
    "                        info = [i, a_dec, a_min, eps, eps_dec, reward, \n",
    "                                q.time, q.policy, q.V, rews]\n",
    "                        \n",
    "                        df_length = len(q_df)\n",
    "                        q_df.loc[df_length] = info\n",
    "    return q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alpha_decs = [0.99, 0.999]\n",
    "alpha_mins =[0.001, 0.0001]\n",
    "eps = [10.0, 1.0]\n",
    "eps_dec = [0.99, 0.999]\n",
    "iters = [1000000, 10000000]\n",
    "q_df = trainQ(P, R, discount=0.9, alpha_dec=alpha_decs, alpha_min=alpha_mins, \n",
    "            epsilon=eps, epsilon_decay=eps_dec, n_iter=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.groupby(\"Iterations\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.groupby(\"Epsilon Decay\").mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
