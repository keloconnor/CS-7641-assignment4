{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.frozen_lake import generate_random_map, FrozenLakeEnv\n",
    "np.random.seed(2)\n",
    "sixteen = generate_random_map(16)\n",
    "np.random.seed(44)\n",
    "tvelve = generate_random_map(12)\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"14x14\": [\n",
    "        \"SFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFH\",\n",
    "        \"FFFFFHHFFFFFFH\",\n",
    "        \"FFFHFFFFFFFFFF\",\n",
    "        \"FHFFFFFHHFFFFF\",\n",
    "        \"FFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFH\",\n",
    "        \"FFFFFHHFFFFFFH\",\n",
    "        \"FFFHFFFFFFFFFF\",\n",
    "        \"FHFFFFFHHFFFFF\",\n",
    "        \"FFFFFFFFFFFFFF\",\n",
    "        \"HFFFFFFFFFFFFG\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, n_epoch=1000):\n",
    "    rewards = []\n",
    "    episode_counts = []\n",
    "    for i in range(n_epoch):\n",
    "        current_state = env.reset()\n",
    "        ep = 0\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done and ep < 1000:\n",
    "            ep += 1\n",
    "            act = int(policy[current_state])\n",
    "            new_state, reward, done, _ = env.step(act)\n",
    "            episode_reward += reward\n",
    "            current_state = new_state\n",
    "        rewards.append(episode_reward)\n",
    "        episode_counts.append(ep)\n",
    "    \n",
    "    mean_reward = sum(rewards)/len(rewards)\n",
    "    mean_eps = sum(episode_counts)/len(episode_counts)\n",
    "    return mean_reward, mean_eps, rewards, episode_counts\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration(env, discount=0.9, epsilon=1e-12):\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    policy = np.zeros((1, number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = 1\n",
    "    sigma = discount\n",
    "    while max_change > epsilon:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = old_value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward \n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += cand_value*prob \n",
    "                        \n",
    "                if total_cand_value > assigned_value:\n",
    "                    assigned_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "                    value_list[0][s] = assigned_value\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "        \n",
    "    end = timer()\n",
    "    time_spent = timedelta(seconds=end-start)\n",
    "    print(\"Solved in: {} episodes and {} seconds\".format(episode, time_spent))\n",
    "    return policy[0], episode, time_spent\n",
    "\n",
    "\n",
    "\n",
    "def policy_iteration(env, discount=0.9, epsilon=1e-3):\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    policy = np.random.randint(number_of_actions, size=(1,number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    episode = 0\n",
    "    sigma = discount\n",
    "    \n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                v = value_list[0][s]\n",
    "\n",
    "                a = policy[0][s]\n",
    "                total_val_new_state = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward                     \n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_val_new_state += cand_value*prob \n",
    "                value_list[0][s] = total_val_new_state\n",
    "                    \n",
    "                eps = max(eps, np.abs(v-value_list[0][s]))\n",
    "            if eps < epsilon:\n",
    "                eval_acc = False\n",
    "\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "\n",
    "            old_action = policy[0][s]\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += prob*cand_value\n",
    "                if total_cand_value > max_value:\n",
    "                    max_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "\n",
    "            if old_action != policy[0][s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    \n",
    "    end = timer()\n",
    "    time_spent = timedelta(seconds=end-start)\n",
    "    print(\"Solved in: {} episodes and {} seconds\".format(episode, time_spent))\n",
    "    return policy[0], episode, time_spent\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test_pi_vi(env, discount=[0.9], epsilon=[1e-9], mute=False):\n",
    "    \n",
    "    vi_dict = {}\n",
    "    \n",
    "    for dis in discount:\n",
    "        vi_dict[dis] = {}\n",
    "        for eps in epsilon:\n",
    "            vi_dict[dis][eps] = {}\n",
    "            \n",
    "            vi_policy, vi_solve_iter, vi_solve_time = value_iteration(env, dis, eps)\n",
    "            vi_mrews, vi_meps, _, __ = test_policy(env, vi_policy)    \n",
    "            vi_dict[dis][eps][\"mean_reward\"] = vi_mrews\n",
    "            vi_dict[dis][eps][\"mean_eps\"] = vi_meps\n",
    "            vi_dict[dis][eps][\"iteration\"] = vi_solve_iter\n",
    "            vi_dict[dis][eps][\"time_spent\"] = vi_solve_time\n",
    "            vi_dict[dis][eps][\"policy\"] = vi_policy\n",
    "            if not mute:\n",
    "                print(\"Value iteration for {} discount and {} eps is done\".format(dis, eps))\n",
    "                print(\"Iteration: {} time: {}\".format(vi_solve_iter, vi_solve_time))\n",
    "                print(\"Mean reward: {} - mean eps: {}\".format(vi_mrews, vi_meps))\n",
    "    pi_dict = {}\n",
    "    for dis in discount:\n",
    "        pi_dict[dis] = {}\n",
    "        for eps in epsilon:\n",
    "            pi_dict[dis][eps] = {}\n",
    "\n",
    "            pi_policy, pi_solve_iter, pi_solve_time = policy_iteration(env, dis, eps)\n",
    "            pi_mrews, pi_meps, _, __ = test_policy(env, pi_policy)    \n",
    "            pi_dict[dis][eps][\"mean_reward\"] = pi_mrews\n",
    "            pi_dict[dis][eps][\"mean_eps\"] = pi_meps\n",
    "            pi_dict[dis][eps][\"iteration\"] = pi_solve_iter\n",
    "            pi_dict[dis][eps][\"time_spent\"] = pi_solve_time\n",
    "            pi_dict[dis][eps][\"policy\"] = pi_policy\n",
    "            if not mute:\n",
    "                print(\"Policy iteration for {} discount is done\".format(dis))\n",
    "                print(\"Iteration: {} time: {}\".format(pi_solve_iter, pi_solve_time))\n",
    "                print(\"Mean reward: {} - mean eps: {}\".format(pi_mrews, pi_meps))\n",
    "\n",
    "    \n",
    "    return vi_dict, pi_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def map_discretize(the_map):\n",
    "    size = len(the_map)\n",
    "    dis_map = np.zeros((size,size))\n",
    "    for i, row in enumerate(the_map):\n",
    "        for j, loc in enumerate(row):\n",
    "            if loc == \"S\":\n",
    "                dis_map[i, j] = 0\n",
    "            elif loc == \"F\":\n",
    "                dis_map[i, j] = 0\n",
    "            elif loc == \"H\":\n",
    "                dis_map[i, j] = -1\n",
    "            elif loc == \"G\":\n",
    "                dis_map[i, j] = 1\n",
    "    return dis_map\n",
    "\n",
    "\n",
    "def policy_numpy(policy):\n",
    "    size = int(np.sqrt(len(policy)))\n",
    "    pol = np.asarray(policy)\n",
    "    pol = pol.reshape((size, size))\n",
    "    return pol\n",
    "\n",
    "\n",
    "def see_policy(map_size, policy):\n",
    "    map_name = str(map_size)+\"x\"+str(map_size)\n",
    "    data = map_discretize(MAPS[map_name])\n",
    "    np_pol = policy_numpy(policy)\n",
    "    plt.imshow(data, interpolation=\"nearest\")\n",
    "\n",
    "    for i in range(np_pol[0].size):\n",
    "        for j in range(np_pol[0].size):\n",
    "            arrow = '\\u2190'\n",
    "            if np_pol[i, j] == 1:\n",
    "                arrow = '\\u2193'\n",
    "            elif np_pol[i, j] == 2:\n",
    "                arrow = '\\u2192'\n",
    "            elif np_pol[i, j] == 3:\n",
    "                arrow = '\\u2191'\n",
    "            text = plt.text(j, i, arrow,\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def plot_the_dict(dictionary, value=\"Score\", size=4, variable=\"Discount Rate\", log=False):\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    title = \"Average and Max {} on {}x{} Frozen Lake\".format(value, size, size)\n",
    "    the_val = value\n",
    "    value = \"Average {}\".format(the_val)\n",
    "    val_type = \"Type of {}\".format(the_val)\n",
    "    the_df = pd.DataFrame(columns=[variable, value, val_type])\n",
    "    for k, v in dictionary.items():\n",
    "        for val in v:\n",
    "            if not log:\n",
    "                dic = {variable: k, value: float(val), val_type: \"Average with std\"}\n",
    "            else:\n",
    "                dic = {variable: np.log10(k), value: float(val), val_type: \"Average with std\"}                \n",
    "            the_df = the_df.append(dic, ignore_index=True)\n",
    "        if not log:\n",
    "            dic = {variable: k, value: float(max(v)), val_type: \"Max\"}\n",
    "        else:\n",
    "            dic = {variable: np.log10(k), value: float(max(v)), val_type: \"Max\"}\n",
    "        the_df = the_df.append(dic, ignore_index=True)\n",
    "    sns.lineplot(x=variable, y=value, hue=val_type, style=val_type, markers=True, data=the_df).set(title=title)\n",
    "\n",
    "    \n",
    "    \n",
    "def convert_dict_to_dict(the_dict):\n",
    "    \n",
    "    discount_rewards = {}\n",
    "    discount_iterations = {}\n",
    "    discount_times = {}\n",
    "\n",
    "\n",
    "    for disc in the_dict:\n",
    "        discount_rewards[disc] = []    \n",
    "        discount_iterations[disc] = []    \n",
    "        discount_times[disc] = []\n",
    "\n",
    "        for eps in the_dict[disc]:\n",
    "            discount_rewards[disc].append(the_dict[disc][eps]['mean_reward'])\n",
    "            discount_iterations[disc].append(the_dict[disc][eps]['iteration'])        \n",
    "            discount_times[disc].append(the_dict[disc][eps]['time_spent'].total_seconds())  \n",
    "\n",
    "            \n",
    "    epsilon_rewards = {}\n",
    "    epsilon_iterations = {}\n",
    "    epsilon_times = {}\n",
    "    for eps in the_dict[0.5]:\n",
    "        epsilon_rewards[eps] = []    \n",
    "        epsilon_iterations[eps] = []    \n",
    "        epsilon_times[eps] = []\n",
    "    \n",
    "        for disc in vi_dict:\n",
    "            epsilon_rewards[eps].append(the_dict[disc][eps]['mean_reward'])\n",
    "            epsilon_iterations[eps].append(the_dict[disc][eps]['iteration'])        \n",
    "            epsilon_times[eps].append(the_dict[disc][eps]['time_spent'].total_seconds()) \n",
    "            \n",
    "    return discount_rewards, discount_iterations, discount_times, epsilon_rewards, epsilon_iterations, epsilon_times\n",
    "\n",
    "\n",
    "def train_and_test_q_learning(env, discount=[0.9], total_episodes=[1e5], alphas=[0.1], decay_rates=[0.01], mute=False):\n",
    "    \n",
    "    min_epsilon = 0.01\n",
    "    \n",
    "    q_dict = {}\n",
    "    for dis in discount:\n",
    "        q_dict[dis] = {}\n",
    "        for eps in total_episodes:\n",
    "            q_dict[dis][eps] = {}\n",
    "            for alpha in alphas:\n",
    "                q_dict[dis][eps][alpha] = {}\n",
    "                for dr in decay_rates:\n",
    "                    q_dict[dis][eps][alpha][dr] = {}\n",
    "                    \n",
    "                    q_policy, q_solve_iter, q_solve_time, q_table, rewards = q_learning(env, dis, eps, alpha, dr, min_epsilon)\n",
    "                    q_mrews, q_meps, _, __ = test_policy(env, q_policy)\n",
    "                    q_dict[dis][eps][alpha][dr][\"mean_reward\"] = q_mrews\n",
    "                    q_dict[dis][eps][alpha][dr][\"mean_eps\"] = q_meps\n",
    "                    q_dict[dis][eps][alpha][dr][\"q-table\"] = q_table\n",
    "                    q_dict[dis][eps][alpha][dr][\"rewards\"] = rewards \n",
    "                    q_dict[dis][eps][alpha][dr][\"iteration\"] = q_solve_iter\n",
    "                    q_dict[dis][eps][alpha][dr][\"time_spent\"] = q_solve_time\n",
    "                    q_dict[dis][eps][alpha][dr][\"policy\"] = q_policy\n",
    "                    if not mute:\n",
    "                        print(\"gamma: {} total_eps: {} lr: {}, dr: {}\".format(dis, eps, alpha, dr))\n",
    "                        print(\"Iteration: {} time: {}\".format(q_solve_iter, q_solve_time))\n",
    "                        print(\"Mean reward: {} - mean eps: {}\".format(q_mrews, q_meps))\n",
    "    return q_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = FrozenLakeEnv(desc=MAPS[\"4x4\"])\n",
    "vi_dict, pi_dict = train_and_test_pi_vi(env, discount=[0.5, 0.75, 0.9, 0.95, 0.99, 0.9999], \n",
    "                                        epsilon=[1e-3, 1e-6, 1e-9, 1e-12, 1e-15], mute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_dict[0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_dict[0.9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = vi_dict[0.99][1e-15]['policy']\n",
    "vi4 = convert_dict_to_dict(vi_dict)\n",
    "see_policy(4, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi4[0], value=\"Score\", size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi4[1], value=\"Iteration\", size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi4[2], value=\"Time\", size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi4[3], value=\"Score\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi4[4], value=\"Iteration\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi4[5], value=\"Time\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = pi_dict[0.99][1e-12]['policy']\n",
    "pi4 = convert_dict_to_dict(pi_dict)\n",
    "see_policy(4, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi4[0], value=\"Score\", size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi4[1], value=\"Iteration\", size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi4[2], value=\"Time\", size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi4[3], value=\"Score\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi4[4], value=\"Iteration\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi4[5], value=\"Time\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = FrozenLakeEnv(desc=MAPS[\"14x14\"])\n",
    "vi_dict14, pi_dict14 = train_and_test_pi_vi(env, discount=[0.5, 0.75, 0.9, 0.95, 0.99, 0.9999], \n",
    "                                          epsilon=[1e-3, 1e-6, 1e-9, 1e-12, 1e-15], mute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = vi_dict14[0.9999][1e-15]['policy']\n",
    "vi14 = convert_dict_to_dict(vi_dict14)\n",
    "see_policy(14, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi14[0], value=\"Score\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi14[3], value=\"Score\", size=4, variable=\"Log Epsilon Value\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi14[1], value=\"Iteration\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(vi14[2], value=\"Time\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi14 = convert_dict_to_dict(pi_dict14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = pi_dict14[0.99][1e-12]['policy']\n",
    "pi14 = convert_dict_to_dict(pi_dict)\n",
    "see_policy(14, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi14[1], value=\"Iteration\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi14[2], value=\"Time\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_dict(pi14[0], value=\"Score\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = FrozenLakeEnv(desc=MAPS[\"4x4\"])\n",
    "episodes = [1e4, 1e5, 1e6]\n",
    "decays = [1e-6]\n",
    "\n",
    "q_dict = train_and_test_q_learning(env, discount=[0.75, 0.9, 0.99, 0.9999], total_episodes=episodes,\n",
    "                          alphas=[0.01, 0.1], decay_rates=decays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = q_dict[0.99][int(1e6)][0.1][1e-06]['policy']\n",
    "see_policy(4, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "episodes = [1e4, 1e5, 1e6]\n",
    "decays = [1e-3, 1e-5]\n",
    "q_dict = train_and_test_q_learning(env, discount= [0.9999], total_episodes=episodes,\n",
    "                          alphas=[0.1, 0.01], decay_rates=decays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rews = q_dict[0.9999][int(1e6)][0.1][1e-03]['rewards']\n",
    "run = 1000\n",
    "rew_running = running_mean(rews, run)\n",
    "indices = [i+run for i in list(range(len(rew_running)))]\n",
    "sns.lineplot(np.log10(indices), rew_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rews = q_dict[0.9999][int(1e6)][0.01][1e-03]['rewards']\n",
    "run = 1000\n",
    "rew_running = running_mean(rews, run)\n",
    "indices = [i+run for i in list(range(len(rew_running)))]\n",
    "sns.lineplot(np.log10(indices), rew_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rews = q_dict[0.9999][int(1e6)][0.01][1e-05]['rewards']\n",
    "run = 1000\n",
    "rew_running = running_mean(rews, run)\n",
    "indices = [i+run for i in list(range(len(rew_running)))]\n",
    "sns.lineplot(np.log10(indices), rew_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "q4 = dict_to_df(q_dict)\n",
    "pl = sns.lineplot(x=\"Training Episodes\", y=\"Reward\", data=q4)\n",
    "pl.figure.set_figwidth(12)\n",
    "pl.figure.set_figheight(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "env = FrozenLakeEnv(desc=MAPS[\"14x14\"])\n",
    "episodes = [1e4, 1e5, 1e6]\n",
    "decays = [1e-3, 1e-5]\n",
    "q_dict16 = train_and_test_q_learning(env, discount= [0.9999], total_episodes=episodes,\n",
    "                          alphas=[0.1, 0.01], decay_rates=decays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pol = q_dict14[0.9999][int(1e6)][0.1][1e-05]['policy']\n",
    "see_policy(14, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rews = q_dict14[0.9999][int(1e6)][0.1][1e-03]['rewards']\n",
    "run = 1000\n",
    "rew_running = running_mean(rews, run)\n",
    "indices = [i+run for i in list(range(len(rew_running)))]\n",
    "sns.lineplot(np.log10(indices), rew_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rews = q_dict14[0.9999][int(1e6)][0.01][1e-03]['rewards']\n",
    "run = 1000\n",
    "rew_running = running_mean(rews, run)\n",
    "indices = [i+run for i in list(range(len(rew_running)))]\n",
    "sns.lineplot(np.log10(indices), rew_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "q14 = dict_to_df(q_dict16)\n",
    "pl = sns.lineplot(x=\"Training Episodes\", y=\"Reward\", data=q16)\n",
    "pl.figure.set_figwidth(12)\n",
    "pl.figure.set_figheight(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
